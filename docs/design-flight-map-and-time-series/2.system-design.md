
# 2. **System Design**

Here we present the overall system data flow diagram displaying each of the proposed components as well as the data flow between them. The remainder of this document will focus on the details of this communication in its entirety. 

![Architectural Overview](./images/architectural-overview.png)

Since we are acquiring data from a data source in a piecemeal fashion (one message per aircraft) at a rate of one messages every 5 seconds, we need a way of collecting these messages into an asynchronous processing queue. [Apache Kakfa](./6.technologies-and-tools.md#apache-kafka) is specifically designed for this purpose. Ingesting flight data into [Apache Kakfa](./6.technologies-and-tools.md#apache-kafka) before transforming and feeding it into other data storage solutions is important because Kafka provides a scalable and fault-tolerant system for handling data streams. Flight data, which is continuously updated from numerous sources, can be unpredictable and require low-latency processing. By using Kafka as an intermediary, we decouple the data ingestion process from the downstream systems, allowing us to buffer the data asynchronously. Any problems, therefore, with data transformations or database connectivity do not cause a loss of data. 


Kafka topics are logical channels that organize and categorize messages (or records) in **Apache Kafka**. Each topic acts as a feed where producers send data and consumers read from. To ensure scalability and efficient parallel processing, Kafka topics are divided into **partitions**. Partitions are essentially ordered, immutable sequences of records, where each record is assigned an offset (a unique identifier). Kafka stores partitions across multiple brokers in a Kafka cluster, enabling parallel reads and writes, distributing the workload, and ensuring fault tolerance. Partitions allow Kafka to achieve horizontal scalability, as each consumer in a consumer group can read from different partitions, enabling distributed processing of large data streams.

Choosing a partition key in Kafka is important because it determines how records are distributed across partitions, directly affecting data locality, load balancing, and performance. A well-chosen partition key ensures that related data is consistently sent to the same partition (ensuring message ordering for that key) and prevents uneven distribution of messages, which could lead to performance bottlenecks if some partitions become overloaded while others remain underutilized.



Next, we will need a consistent internal schema definition that our internal services and applications can reference and process the data accordingly. 

Based on your specifications, here's the **Protobuf schema** for the flight data, including fields like flight ID, location, altitude, airspeed, timestamp, and metadata (airline, flight number, departure, and arrival airports):

```proto
syntax = "proto3";

message FlightData {
  // Unique flight identifier
  string flight_id = 1;

  // Geospatial location data (latitude, longitude)
  Location location = 2;

  // Altitude in feet
  int32 altitude = 3;

  // Airspeed in knots
  int32 airspeed = 4;

  // Timestamp in ISO 8601 format or UNIX epoch time
  string timestamp = 5;

  // Metadata: airline, flight number, departure and arrival airports
  FlightMetadata metadata = 6;
}

// Location schema (latitude, longitude)
message Location {
  double latitude = 1;
  double longitude = 2;
}

// Metadata schema: additional flight details
message FlightMetadata {
  string airline = 1;
  string flight_number = 2;
  string departure_airport = 3;
  string arrival_airport = 4;
}
```

### **Explanation of the Protobuf Fields:**

1. **FlightData**:
   - Contains the main flight data, such as the flight ID, location, altitude, airspeed, timestamp, and metadata (airline, flight number, departure, and arrival airports).

2. **Location**:
   - Stores the latitude and longitude of the aircraft.

3. **Metadata**:
   - Holds additional flight information like the airline name, flight number, and the codes for the departure and arrival airports.

### **Field Descriptions**:
- `flight_id`: A unique identifier for each flight.
- `location`: A nested message containing the `latitude` and `longitude` of the aircraft.
- `altitude`: The current altitude of the aircraft in feet.
- `airspeed`: The current airspeed of the aircraft in knots.
- `timestamp`: The time when the position and data were recorded, represented as a string (ISO 8601 format or UNIX timestamp).
- `metadata`: A nested message containing flight-related information such as the airline, flight number, and airports (departure and arrival).

This schema is lightweight and can efficiently represent and transfer the flight data across systems using Protobuf serialization.

Describe message format schema format 

The data schema for flight data should capture the essential attributes of each flight, including its metadata, real-time position, and flight metrics (e.g., altitude and speed). The schema must be flexible to accommodate frequent updates and ensure that the data is structured for efficient processing, querying, and storage in systems like Kafka, Elasticsearch, or time-series databases.


### **Key Design Considerations:**

- **Real-time updates**: Since flight data is dynamic, the `position` and `status` fields will likely update frequently. This structure allows for easily updating the position data with each new reading (e.g., every 5 seconds).
- **Temporal data**: The `timestamp` field in the `position` object captures when the position data was recorded. This is critical for tracking the flight's progression over time.
- **Flexible storage**: The schema is flexible enough to be stored in a variety of systems (Kafka, Elasticsearch, Redis, etc.), and can be enriched or augmented as needed (e.g., adding additional fields for analytics).

### **Partitioning in Kafka:**
In Kafka, you might partition the data using the **flight_id** or **airline** so that all updates for the same flight are processed sequentially, ensuring that real-time position updates for each flight maintain consistency.

### **Schema Evolution:**
This schema can be extended with more fields or attributes (e.g., adding in-flight entertainment data or cabin conditions) as needed without breaking existing processes. This flexibility is key to handling future data requirements or integrating additional data sources.


Describe how general flight message data is ingested into kafka pipeline



3. **Flight Data Producer Example in Python:**
   Below is an example of a Python Kafka producer that takes flight data (in JSON format) and sends it to a Kafka topic.

   ```python
   import json
   from kafka import KafkaProducer
   from datetime import datetime

   # Initialize the Kafka producer
   producer = KafkaProducer(
       bootstrap_servers=['localhost:9092'],  # Kafka broker URL
       value_serializer=lambda v: json.dumps(v).encode('utf-8'),  # JSON serializer
       key_serializer=str.encode  # String key serializer
   )

   # Function to send flight data to Kafka
   def send_flight_data(flight_data):
       flight_id = flight_data['flight_id']
       producer.send('flights-topic', key=flight_id, value=flight_data)
       producer.flush()  # Ensure the message is sent to Kafka

   # Example flight data (you would collect this from an external source)
   flight_data = {
       "flight_id": "AB123",
       "airline": "Airline ABC",
       "flight_number": "AB123",
       "aircraft": {
           "tail_number": "N123AB",
           "model": "Boeing 737",
           "type": "Commercial"
       },
       "origin": {
           "airport_code": "JFK",
           "airport_name": "John F. Kennedy International Airport",
           "latitude": 40.6413111,
           "longitude": -73.7781391,
           "city": "New York",
           "country": "USA"
       },
       "destination": {
           "airport_code": "LAX",
           "airport_name": "Los Angeles International Airport",
           "latitude": 33.9415889,
           "longitude": -118.40853,
           "city": "Los Angeles",
           "country": "USA"
       },
       "status": "en route",
       "position": {
           "latitude": 39.123456,
           "longitude": -77.987654,
           "altitude": 35000,
           "speed": 450,
           "heading": 90,
           "timestamp": datetime.utcnow().isoformat()
       },
       "departure": {
           "scheduled": "2024-10-01T14:00:00Z",
           "actual": "2024-10-01T14:15:00Z"
       },
       "arrival": {
           "scheduled": "2024-10-01T17:30:00Z",
           "estimated": "2024-10-01T17:20:00Z"
       },
       "weather": {
           "wind_speed": 15,
           "wind_direction": 270,
           "visibility": 10,
           "conditions": "Clear"
       }
   }

   # Send the flight data to Kafka
   send_flight_data(flight_data)

   # Close the Kafka producer connection
   producer.close()
   ```

### **Explanation of the Code:**

1. **KafkaProducer**:
   - The `KafkaProducer` is initialized with the Kafka broker URL (`localhost:9092` in this case). It includes a `value_serializer` that serializes the Python dictionary into a JSON string before sending it to Kafka. The `key_serializer` serializes the flight ID as a string.

2. **Flight Data**:
   - The flight data is structured as a JSON object. This data will be passed to the `send_flight_data` function, which sends it to the `flights-topic` in Kafka. The `flight_id` is used as the message key to ensure that all updates for the same flight are written to the same Kafka partition.

3. **Producer.send**:
   - The `send` method of the Kafka producer sends the message to the Kafka topic (`flights-topic`). The key (`flight_id`) ensures that messages are partitioned correctly.

4. **Producer.flush**:
   - The `flush()` method ensures that all buffered messages are sent to Kafka before continuing.

5. **Data Example**:
   - A sample flight data schema is provided in JSON format, which includes information like flight position, altitude, speed, origin, destination, and weather conditions.

### **Kafka Topic Configuration**:
Ensure that the Kafka topic (`flights-topic`) is created and configured to handle the required throughput. You might want to configure the topic with multiple partitions to handle large-scale data ingestion effectively.

```bash
kafka-topics.sh --create --topic flights-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

### **Potential Extensions**:
- **Error Handling**: You could add more robust error handling and retries in case of Kafka connectivity issues.
- **Batching**: For efficiency, you may want to batch multiple messages together before sending them to Kafka.
- **Enrichment**: If needed, the flight data can be enriched before publishing, for example by adding additional weather or traffic data.

### **Conclusion**:
This Python-based Kafka producer service allows you to efficiently ingest flight data in real-time, converting it into JSON format and sending it to Kafka for downstream processing. This setup can scale horizontally by adding more producer instances to handle a higher throughput of flight data ingestion.
