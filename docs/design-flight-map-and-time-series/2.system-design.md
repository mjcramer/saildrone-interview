
# 2. **System Design**

The data flow diagram below displays the major system components as well as the data flow between them. Here we will discuss the functions of these components, paying special attention to the particular strengths of each that make it particularly suitable for this application's feature set. Later sections of this document will focus on the specific details of each feature, including the models and protocols involved, as well as other important architectural considerations of each component. 

![Architectural Overview](./images/architectural-overview.png)

## Message Queue / Data Pipeline 

Since we are acquiring flight data from a source in a piecemeal fashion (one message per aircraft) at a rate of one message every 5 seconds, we need a way of collecting these messages into an asynchronous processing queue. Flight data which is continuously updated from numerous sources can be unpredictable and we will require low-latency storage or caching on the query side. Any problems with data transformations or database connectivity could cause a loss of data, therefore, it would be advantageous to decouple the data ingestion process from any downstream system, and allow us to buffer the data asynchronously.  [Apache Kakfa](./6.technologies-and-tools.md#apache-kafka) is specifically designed for this purpose. Ingesting flight data into [Apache Kakfa](./6.technologies-and-tools.md#apache-kafka) before transforming and feeding it into other data storage solutions is important because Kafka provides a scalable and fault-tolerant system for handling data streams. By using Kafka as an intermediary, as we can manage the data processing from source separately and effectively cache that data for a limited time in the pipeline. Downstream processes can be restarted and network connectivity can be restored and we can resume processing data without loss.  

### Key Partitioning 

[Apache Kakfa](./6.technologies-and-tools.md#apache-kafka) is a pubish/subsribe system with *topics* that represent logical channels that organize and categorize messages (or records). Each topic acts as a feed to which producers send data and from which consumers read data. To ensure scalability and efficient parallel processing, Kafka topics are divided into *partitions*. Partitions are essentially ordered, immutable sequences of records, where each record is assigned an offset (a non-monitomically increasing integer) as it is published. Kafka stores partitions across multiple servers in a Kafka cluster, enabling parallel reads and writes, distributing the workload, and ensuring fault tolerance. Partitions allow Kafka to achieve horizontal scalability, as multiple consumers in assigned groups can read from different partitions, enabling distributed processing of large data streams.

Choosing a partition key in Kafka is important because not only does it determine how records are distributed across partitions, directly affecting data locality, load balancing, and performance, but also because Kafka can guarantee that messages are ordered within that partition. A well-chosen partition key ensures that related data is consistently sent to the same partition and can therefore be read in consistent order. For this application it is critical that use the `flight_id` as the partition key so that we can ensure that we are always receiving the most recent update for that flight (note: Kafka can only provide this guarantee for the timestamp in which messages are published to Kafka, NOT for any timestamp that may be contained in a message).

### Message Schema

Why did we choose protobuf?

This schema is lightweight and can efficiently represent and transfer the flight data across systems using Protobuf serialization.

The data schema for flight data should capture the essential attributes of each flight, including its metadata, real-time position, and flight metrics (e.g., altitude and speed). The schema must be flexible to accommodate frequent updates and ensure that the data is structured for efficient processing, querying, and storage in systems like Kafka, Elasticsearch, or time-series databases.



```proto
syntax = "proto3";

message FlightData {
  // Unique flight identifier
  string flight_id = 1;

  // Geospatial location data (latitude, longitude)
  Location location = 2;

  // Altitude in feet
  int32 altitude = 3;

  // Airspeed in knots
  int32 airspeed = 4;

  // Timestamp in ISO 8601 format or UNIX epoch time
  string timestamp = 5;

  // Metadata: airline, flight number, departure and arrival airports
  FlightMetadata metadata = 6;
}

// Location schema (latitude, longitude)
message Location {
  double latitude = 1;
  double longitude = 2;
}

// Metadata schema: additional flight details
message FlightMetadata {
  string airline = 1;
  string flight_number = 2;
  string departure_airport = 3;
  string arrival_airport = 4;
}
```

### Kafka Consumers

Do I need this?

To consume and process messages in Kafka using Python and Protocol Buffers (`protobuf`), you'll need to generate the Python classes from the `.proto` schema using `protoc`:

```bash
protoc --python_out=. flight_data.proto
```

This will generate a `flight_data_pb2.py` file that contains the `FlightData`, `Location`, and `FlightMetadata` classes.

#### Kafka Consumer Example Using `kafka-python`

Here’s the Python code to consume and deserialize Protocol Buffers messages using the `kafka-python` library:

```python
from kafka import KafkaConsumer
# The generated classes from the protobuf messages
from flight_data_pb2 import FlightData

# Create Kafka consumer
# We configure the Kafka consumer with settings like `bootstrap_servers` (Kafka broker address), # #`group_id` (the consumer group), and `auto_offset_reset` (to start reading from the earliest offset).

consumer = KafkaConsumer(
    KAFKA_TOPIC,
    bootstrap_servers=[KAFKA_BROKER],
    group_id=KAFKA_GROUP_ID,
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    # Deserialize protobuf
#    - The `value_deserializer` is set to a lambda function that uses the `FromString` method of the generated `FlightData` class to deserialize the byte array from Kafka into a `FlightData` object.
    value_deserializer=lambda x: FlightData().FromString(x) 
)

# Main loop to consume messages from Kafka
try:
    print("Consuming messages from Kafka...")
    # The consumer runs in an infinite loop, fetching messages from the specified Kafka topic and passing # them to the `process_flight_data` function for handling.
   for message in consumer:
        # Deserialize and process the flight data
        process_flight_data(message.value)

except KeyboardInterrupt:
    print("Stopping consumer...")
finally:
    consumer.close()
```

## `Kubernetes` for Container Orchestration

Choosing **Kubernetes** to orchestrate containers in this flight tracking system offers robust scalability, high availability, and automated management of the system's microservices architecture. Kubernetes provides powerful features for deploying, managing, and scaling containerized applications, which is crucial for a real-time system that processes dynamic flight data and supports live maps and historical data analysis. With Kubernetes, you can automatically scale services like real-time flight tracking, historical data storage, and geospatial queries based on traffic demands, ensuring that the system remains responsive even during peak loads, such as when users are querying live flight positions. It also offers automatic load balancing across containers, distributing requests to ensure optimal resource utilization and performance.

Additionally, Kubernetes provides built-in fault tolerance and self-healing capabilities, which are essential for maintaining high availability in a system that needs to operate 24/7. If a container fails, Kubernetes will automatically restart it, ensuring minimal downtime. Kubernetes also simplifies rolling updates and canary deployments, allowing you to deploy new features or updates without affecting the system’s availability, which is critical when managing real-time data streams and analytics. Its declarative configuration model and support for infrastructure-as-code make it easier to manage complex multi-container environments, integrate CI/CD pipelines, and ensure consistency across environments. This orchestration layer is particularly beneficial in a distributed system where multiple services must work together seamlessly to provide real-time flight tracking and historical data analysis at scale.


## `Tile38` for Real-Time Geospatial Querying

Choosing **Tile38** as a real-time storage solution for the live map portion of a flight tracking system offers significant advantages due to its ability to handle large-scale geospatial data efficiently. Tile38 excels in real-time applications where frequent updates and low-latency queries are critical, such as a live map displaying the current positions of flights. It supports geospatial indexing and querying, allowing for quick retrieval of flights within specific geographic areas, such as bounding boxes or proximity searches, which is essential for dynamically updating maps as users pan or zoom. Its in-memory architecture ensures that real-time updates, like flight position changes, are processed and retrieved almost instantaneously, providing users with a smooth and responsive map experience.


- **Tile38**: Chosen due to their ability to handle high-throughput and low-latency access. It provides fast in-memory storage and can handle bounding box queries natively. 



Additionally, Tile38's support for advanced geofencing makes it ideal for flight tracking, as it can trigger events when flights enter or exit specific airspace regions, enabling real-time notifications and alerts. The Redis-like interface simplifies integration into existing systems, making it easy to implement in a microservices architecture. Tile38 is built to scale, allowing the system to handle large numbers of flights, frequent updates, and complex spatial queries with high efficiency. This combination of real-time data handling, geospatial indexing, and scalability makes Tile38 an excellent choice for the live map component of a flight tracking system where real-time accuracy and performance are paramount.


## `Postgres` with `TimescaleDB` extension for Geospatial Time Series Data Storage

Choosing **PostgreSQL with TimescaleDB extensions** for the historical flight data portion of a flight tracking system is ideal due to its ability to handle time-series data efficiently while providing the robustness and flexibility of a relational database. TimescaleDB extends PostgreSQL to offer specialized time-series data features such as automatic data partitioning (hypertables), fast aggregation, and optimized storage for time-ordered data. This makes it perfect for storing large volumes of historical flight data, which includes continuous streams of location updates, altitude, speed, and other flight metrics over time. With TimescaleDB, queries that involve time-based filtering, such as retrieving a flight’s path over a specific date range or analyzing trends in flight patterns, are significantly accelerated, ensuring fast performance even with large datasets.

Furthermore, PostgreSQL’s native support for ACID transactions, complex queries, and indexing, combined with TimescaleDB's optimizations for time-series data, ensures that historical flight data remains accurate, consistent, and easily accessible for analysis. The integration of TimescaleDB allows the system to benefit from advanced features such as data retention policies, which can automatically archive or downsample older data to save storage costs while keeping recent data highly detailed. Additionally, PostgreSQL’s extensibility with tools like PostGIS (for geospatial data) means that historical flight data can be queried both spatially and temporally, providing powerful capabilities for flight path analysis, historical reporting, and forecasting. This makes PostgreSQL with TimescaleDB an excellent choice for managing, querying, and analyzing historical flight data in a scalable and efficient manner.
