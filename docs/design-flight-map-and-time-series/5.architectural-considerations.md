# 5. Architectural Considerations

In this section we will discuss the specific details of each component regarding a particular concern: scalability, latency, resiliancy, data integrity, security, monitoring and alerting.
   
## 5.1. Scalability

### 5.1.1 Kafka
Kafka can be scaled by adding more brokers. More partitions can be also added but this is requires a bit of extra effort and it's best to choose an optimal number of partition up front for the foreseeable lifetime of the system. Kafka consumers can be scaled by adding additional consumers up to the number of partions available in a topic. For each consumer in an identifiable group, that consumer is assigned one or more partitions and will only receive data from that partition. Kafka consumer clients also provide low level interfaces for custom partition management. 

### 5.1.2 Tile38

One way we can scale `Tile38` is by creating multiple instances of the `tile38-server` and partitioning the data set by flight id. This will ensure that flights all over the world would be spread across a number of partitions evenly and we could aggregate the the partitions on the client side very simply by combining them. `Tile38` supports clustering and sharding to scale horizontally, distributing the data across multiple machines.

### 5.1.3 Postgres

PostgreSQL supports scaling through both vertical and horizontal approaches. Vertical scaling involves increasing the resources (CPU, memory, disk space) of a single machine to handle larger workloads. Horizontal scaling, or scaling out, can be achieved using replication, sharding, or partitioning. Replication allows for read scaling by distributing read queries across multiple standby servers, while a primary server handles writes. Sharding distributes data across multiple nodes to balance the load. Additionally, partitioning large tables into smaller, more manageable chunks can improve query performance as well.

### 5.1.1 Load Balancers

Use load balancers for both active and historical API services to spread requests among multiple instances. If `Tile38` sharding is needed, one useful strategy could be to use an internal load balancer 




## 5.2. Latency

Both `Kafka` and `Tile38` are designed for low-latency, high-throughput messaging, typically achieving sub-millisecond latencies. However, actual latency can vary based on factors such as network conditions, message size, server configurations, and the use of acknowledgments or replication settings. Of particular concern would be how the use of load balancers affects latency since we would be created additional hops from source to destination and this might adversely affect our traversal time. Particularly with our use of Kubernetes as an orchestration tool, it may be worth looking into some network optimization tooling or service meshes such as `Linkerd` or `Istio` to provide additional throughput or low-level connectivity.

## 5.3. Resiliancy



- **Redundancy**: The system replicates data across multiple instances of Redis or the time series database to ensure high availability and prevent data loss in the event of node failure.
- **Backpressure**: The stream processing layer will implement backpressure handling to ensure that the system does not become overwhelmed during periods of high traffic or data volume.

## 5.4. Data Integrity

Our use of Protocol Buffers (Protobuf) helps with data integrity and schema evolution in several ways:

 - Protobuf is a binary serialization format that ensures efficient and consistent data encoding across different platforms and languages. It uses a well-defined schema to serialize and deserialize data, reducing the risk of data corruption or misinterpretation during transmission.

 - Protobuf supports forward and backward compatibility, allowing schemas to evolve over time without breaking existing systems. Fields can be added or deprecated without affecting older versions, as unknown fields are safely ignored. Protobuf assigns unique field numbers to each element, ensuring that changes to the schema do not disrupt communication between different versions of services, making it easier to manage long-term service evolution.

## 5.5. Security 

All components are on private VPN with no inbound connectivity allowed, except through VPN tunnels (maybe) and load balancers, which can also serve as the terminus for SSL connections to simplify internal connnections 

- There may be certain compliance standards which may have to be adhered to or followed so some investigation here is definitely warrented.

## 5.6. Monitoring and Alerting

- Must design and implement a system for collecting and visualing metrics 
- Need to be alerted when data is not being ingested from the source
- Need to be alerted 



